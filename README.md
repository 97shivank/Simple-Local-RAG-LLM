# **Simple Local RAG LLM Model**
![RAG Pipeline Workflow](image.png)
An efficient Retrieval-Augmented Generation (RAG) pipeline integrating Google Gemini LLM, Sentence-Transformers, and vector-based search to retrieve and generate contextually relevant answers from PDF documents.

## **Features**
	â€¢	ğŸ“„ PDF Processing: Extracts and tokenizes text from PDFs for chunking.
	â€¢	ğŸ” Vector Embeddings & Search: Uses Sentence-Transformers for document chunk embedding and Approximate Nearest Neighbor (ANN) search.
	â€¢	ğŸ§  Google Gemini LLM Integration: Fine-tuned for accurate and high-quality responses.
	â€¢	ğŸ¤– Retrieval-Augmented Generation (RAG): Enhances LLM-generated answers with retrieved document content for improved relevance.

ğŸ“‚ Installation

git clone [https://github.com/yourusername/Simple-Local-RAG-LLM.git ](https://github.com/97shivank/Simple-Local-RAG-LLM) 
cd Simple-Local-RAG-LLM  
pip install -r requirements.txt

ğŸ“œ Usage

1ï¸âƒ£ Download & Extract Text from PDF

from utils import open_and_read_pdf  
pages_and_texts = open_and_read_pdf("your_document.pdf")  

2ï¸âƒ£ Embed & Index Chunks

from sentence_transformers import SentenceTransformer  
model = SentenceTransformer("all-MiniLM-L6-v2")  
embeddings = model.encode([chunk["text"] for chunk in pages_and_texts])  

3ï¸âƒ£ Query & Retrieve Relevant Chunks

from similarity_search import retrieve_relevant_chunks  
relevant_chunks = retrieve_relevant_chunks(query="What is human nutrition?", embeddings, pages_and_texts)  

4ï¸âƒ£ Generate Responses using Google Gemini LLM

from llm import generate_response  
response = generate_response(relevant_chunks)  
print(response)  

âš™ï¸ Tech Stack
	â€¢	Python, Sentence-Transformers, FAISS, Google Gemini API
	â€¢	RAG (Retrieval-Augmented Generation) Pipeline

ğŸ—ï¸ Future Enhancements
	â€¢	âœ… Support for multiple document formats (TXT, DOCX, HTML).
	â€¢	âœ… UI-based query interface.

ğŸ“œ License

This project is licensed under the MIT License.
