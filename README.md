# **Simple Local RAG LLM Model**
![RAG Pipeline Workflow](image.png)
An efficient Retrieval-Augmented Generation (RAG) pipeline integrating Google Gemini LLM, Sentence-Transformers, and vector-based search to retrieve and generate contextually relevant answers from PDF documents.

## **Features**
	‚Ä¢	üìÑ PDF Processing: Extracts and tokenizes text from PDFs for chunking.
	‚Ä¢	üîç Vector Embeddings & Search: Uses Sentence-Transformers for document chunk embedding and Approximate Nearest Neighbor (ANN) search.
	‚Ä¢	üß† Google Gemini LLM Integration: Fine-tuned for accurate and high-quality responses.
	‚Ä¢	ü§ñ Retrieval-Augmented Generation (RAG): Enhances LLM-generated answers with retrieved document content for improved relevance.

## **Installation**

git clone [https://github.com/97shivank/Simple-Local-RAG-LLM.git]<br>
cd Simple-Local-RAG-LLM  
pip install -r requirements.txt

## **Usage**

1Ô∏è‚É£ Download & Extract Text from PDF

from utils import open_and_read_pdf  
pages_and_texts = open_and_read_pdf("your_document.pdf")  

2Ô∏è‚É£ Embed & Index Chunks

from sentence_transformers import SentenceTransformer  
model = SentenceTransformer("all-MiniLM-L6-v2")  
embeddings = model.encode([chunk["text"] for chunk in pages_and_texts])  

3Ô∏è‚É£ Query & Retrieve Relevant Chunks

from similarity_search import retrieve_relevant_chunks  
relevant_chunks = retrieve_relevant_chunks(query="What is human nutrition?", embeddings, pages_and_texts)  

4Ô∏è‚É£ Generate Responses using Google Gemini LLM

from llm import generate_response  
response = generate_response(relevant_chunks)  
print(response)  

## **Tech Stack**
	‚Ä¢	Python, Sentence-Transformers, FAISS, Google Gemini API
	‚Ä¢	RAG (Retrieval-Augmented Generation) Pipeline

## **Future Enhancements**
	‚Ä¢	‚úÖ Support for multiple document formats (TXT, DOCX, HTML).
	‚Ä¢	‚úÖ UI-based query interface.

## **License**

This project is licensed under the MIT License.
